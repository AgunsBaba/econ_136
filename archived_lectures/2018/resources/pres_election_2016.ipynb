{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qHeYjhjDH1dy"
   },
   "source": [
    "# Predicting the Trump Election: An Introduction to Tensorflow\n",
    "This notebook demonstrates an introduction to Tensorflow for predicting the Trump victory for the 2016 presidential election, using stock market and 3rd party data. Through this guide you will utilize [DNNRegressor](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor) a library in Tensorflow that allows you to quickly build a fully connected neural network and train a model. DNNRegressor is chosen over a more standard classification pipeline as we do not have to 1-hot encode the features. This will allow for our input vectors (i.e. timeseries data), to be more dense.\n",
    "\n",
    "The following steps are performed:\n",
    "1. Upload\n",
    "2. Preprocess timeseries features\n",
    "3. Play/Visualize the data\n",
    "4. Model: Predicting Trump Election\n",
    "5. Model: Predicting the market returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview \n",
    "The goal of this is to predict the winner of the 2016 Presidential Election using publicly available data at the time.  Instead of predicting the person to win we will phrase this problem as a binary classification task: predicting the political party that will win the election (Republican or Democratic). This will give us more data to sample from, hopefully improving the model performance. After that, we will use similar features to train another neural network which will be used to predict the market return after the election date. \n",
    "\n",
    "We have a small amount of data, overfitting and biases are a major problem. In practice you will have much more data. Using a GPU to accelerate the training time will be beneficial. Therefore, this tutorial will give you access to one of Google's GPU's on demand. You will likely use a [Tesla K80 GPU](https://www.nvidia.com/en-us/data-center/tesla-k80/), and old but powerful and expensive GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "G1rV0N5PHNeV"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "scEH8w_pH0fN"
   },
   "source": [
    "### 1. Upload Files\n",
    "Here you will upload two files: data.csv, djw.csv\n",
    "\n",
    "* **data.csv** contains historical data about presidential elections dating back until 1900.\n",
    "* **djw.csv** contains historical data about the [Dow Jones Industrial Average](https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average) for the daily close prices. \n",
    "Although this is an index, it can closely approximate the DIA ETF and other ETF's that track \"the market\". This was chosen due the large dataset size. \n",
    "\n",
    "If you are working within a Colab enviroment you will need to upload them using the command below. If you are working on your desktop you can skip the below step and make sure you reference the files in the workbook correctly. \n",
    "\n",
    "Simply click on the _Choose Files_ and you will be able to upload your files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14844,
     "status": "ok",
     "timestamp": 1523610144497,
     "user": {
      "displayName": "Avi Thaker",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "117542160256971382138"
     },
     "user_tz": 420
    },
    "id": "XdxW5xBVHSGf",
    "outputId": "56f8bf95-d587-4b14-f146-b4ca4d549754"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zxEtHS7yKqNl"
   },
   "source": [
    "### 2. Preprocess Data\n",
    "In this section we preprocess the data. Here we have to be careful that we do not inject forward looking bias. Therefore, we need to use the ```truncate``` command to get the nearest day before. The helper function below shows how to get the percentage return of the market given a dataframe of prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "E5SB0hKVKpGw"
   },
   "outputs": [],
   "source": [
    "def compute_td_pct(djw, index, days):\n",
    "    \"\"\" Computes a percentage change between a given day and some timedelta (days)\n",
    "    Args:\n",
    "        djw(PandasDataframe): contains index of prices and dates\n",
    "        index(datetime): day to search\n",
    "        days(int): numbers of days to search back\n",
    "    Returns:\n",
    "        (pct, int): percent change, and direction (1 positive, 0 negative)\n",
    "    \"\"\"\n",
    "    pct = None\n",
    "    ntd = djw.truncate(after=index).iloc[-1][\"Closing Value\"]\n",
    "    if days > 0:\n",
    "        pct = (djw[index:index+datetime.timedelta(days=1)].iloc[-1][\"Closing Value\"] - ntd) / djw[index:index+datetime.timedelta(days=days)].iloc[-1][\"Closing Value\"]\n",
    "    else:\n",
    "        pct = (ntd - djw[index+datetime.timedelta(days=days):index].iloc[0][\"Closing Value\"]) / ntd\n",
    "    if pct > 0.0:\n",
    "        return pct, 1\n",
    "    else:\n",
    "        return pct, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert times to datetimes for easier processing. Pandas has great built in libraries that allow for quick data parsing. Pandas include a nice helper function called ```.to_datetime()``` which will automatically convert and figure out datetimes for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "vUFkaSm4HTOt"
   },
   "outputs": [],
   "source": [
    "djw = pd.read_csv(\"djw.csv\") # Dow Jones Industrial Average Prices by Day\n",
    "djw = djw.set_index(pd.to_datetime(djw[\"Date\"])) # Set the Datetime as index\n",
    "data = pd.read_csv(\"data.csv\") # Read in 3rd party handlabeled data\n",
    "data = data.set_index(pd.to_datetime(data[\"date_elected\"])) # Set the datetime as the index\n",
    "data = data[1:] # We remove the first index to make sure we have enough data to look backwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FhF4ambrKab5"
   },
   "source": [
    "Label out the features to sample. Here we believe that the market or some combination of the market features may predict the election. Id est: smart money might know where the election may go and invest accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "NYZPW3ApHllq"
   },
   "outputs": [],
   "source": [
    "# This could have been done in a list of lists but was made explicit for demonstration purposes\n",
    "day_before_1 = []   # 1 day before the election \n",
    "day_before_7 = []   # 7 days before the election\n",
    "day_before_30 = []  # 30 days before the election\n",
    "day_before_60 = []  # 60 days before the election\n",
    "day_before_180 = [] # 180 days before the election\n",
    "day_before_365 = [] # 365 days before the election\n",
    "day_before_730 = [] # 730 days before the election\n",
    "day_after_1 = []    # 1 day after the election\n",
    "day_after_7 = []    # 7 days after the election\n",
    "day_after_30 = []   # 30 days after the election\n",
    "day_after_60 = []   # 60 days after the election\n",
    "day_after_180 = []  # 180 days after the election\n",
    "day_after_365 = []  # 365 days after the election\n",
    "for index, row in data.iterrows():\n",
    "    day_after_1.append(compute_td_pct(djw, index, 1)[1]) # Note here we are just getting the direction instead of the market change\n",
    "    day_after_7.append(compute_td_pct(djw, index, 7)[0])\n",
    "    day_after_30.append(compute_td_pct(djw, index, 30)[0])\n",
    "    day_after_60.append(compute_td_pct(djw, index, 60)[0])\n",
    "    day_after_180.append(compute_td_pct(djw, index, 180)[0])\n",
    "    day_after_365.append(compute_td_pct(djw, index, 365)[0])\n",
    "    day_before_1.append(compute_td_pct(djw, index, -1)[0])\n",
    "    day_before_7.append(compute_td_pct(djw, index, -7)[0])\n",
    "    day_before_30.append(compute_td_pct(djw, index, -30)[0])\n",
    "    day_before_60.append(compute_td_pct(djw, index, -60)[0])\n",
    "    day_before_180.append(compute_td_pct(djw, index, -180)[0])\n",
    "    day_before_365.append(compute_td_pct(djw, index, -365)[0])\n",
    "    day_before_730.append(compute_td_pct(djw, index, -730)[0])   \n",
    "    \n",
    "# Finally construct a DataFrame containing all of the data and add column labels and concat\n",
    "# the market data to the third party data\n",
    "market_data_cols = [day_before_1, day_before_7, day_before_30, day_before_60, day_before_180, day_before_365, day_before_730, day_after_1, day_after_7, day_after_30, day_after_60, day_after_180, day_after_365]\n",
    "market_data_col_names = [\"day_before_1\",\"day_before_7\",\"day_before_30\",\"day_before_60\",\"day_before_180\",\"day_before_365\",\"day_before_730\",\"day_after_1\",\"day_after_7\",\"day_after_30\",\"day_after_60\",\"day_after_180\",\"day_after_365\"]\n",
    "market_data = pd.DataFrame(market_data_cols).transpose()\n",
    "market_data.columns = market_data_col_names\n",
    "market_data = market_data.set_index(data.index) # this operation is not inplace, use existing dataframe's index\n",
    "frames = [data, market_data] # Pandas has some quirks unlike sql when concatenating\n",
    "combined_df = pd.concat(frames, axis=1) # Axis 0 is after, 1 is next-to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ue2wRXa_LIHm"
   },
   "source": [
    "### 3. Play/Visualize Data\n",
    "Now that we have preprocessed the data, take a look at the data and get a feel for how it is structured. You will note that there is not that much data, as it is hard to find reliable stock data in the early 1900's. \n",
    "\n",
    "Examine the features to get a sense of what they mean. \n",
    "* Party - 1 if Republican, 0 if Democratic\n",
    "* Previously Held Office - 1 if true\n",
    "* Previous Party - the party that was previously in power (goes back 2 terms), 1 if Republican, 0 if Democratic\n",
    "* Was VP or VP Runner - 1 if held the position of VP before the current election\n",
    "* day_before_n - percentage or direction of the market for a given number of days before the current election cycle but not including the day\n",
    "* day_after_n - percentage or direction of the market for a given number of days after the current election cycle\n",
    "\n",
    "When I actually did the prediction, I had much more data than just the above. I used [Google Trends](trends.google.com) to add more feature data. Furthermore, I added a \"sentiment analysis\" by looking through social media and other documents to get a feeling for the expected outcome. I strongly reccomend you include more features and more data than the 20+ elements we have here. More data the better. High quality data is important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1523610146905,
     "user": {
      "displayName": "Avi Thaker",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "117542160256971382138"
     },
     "user_tz": 420
    },
    "id": "rv1xcxBiLGt3",
    "outputId": "16a49823-8f73-4230-a50c-0f70b0c0c035"
   },
   "outputs": [],
   "source": [
    "combined_df.head() # gives the top 5, can use tail to give the last 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1523610147473,
     "user": {
      "displayName": "Avi Thaker",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "117542160256971382138"
     },
     "user_tz": 420
    },
    "id": "_NeZ8xMvLHiK",
    "outputId": "3e6f5b65-a46d-4957-f93c-f9b9a5534a4d"
   },
   "outputs": [],
   "source": [
    "combined_df.describe() # statistics about the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model: Predicting the Trump Election\n",
    "Here we will train a DNN that aims to predict the 2016 Presidential Election. The features will be the features explored above (except for the forward looking ones). You do not need to fully understand how a [neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network) works, however it can be thought of mapping inputs to outputs and the network will figure out everything inbetween. The aim is to not have the best network architecture possible, but to leverage neural network's ability to find patterns among data that otherwise would be difficult or timeconsuming to find by pure inspection. \n",
    "\n",
    "![Feed Forward Neural Network](https://en.wikipedia.org/wiki/File:Feed_forward_neural_net.gif)\n",
    "\n",
    "We are using Deep Learning to figure out the useful features and generate a model based upon those useful features to predict upon. \n",
    "\n",
    "Tensorflow is the selected Deep Learning framework, as it tends to be the most popular in industry. There are many others and each has a different purpose and use. Use what is best to get the job done.\n",
    "* CNTK (Microsoft Cognitive Toolkit)\n",
    "* Keras - this actually is a high level API that has general calls to other frameworks\n",
    "* Theano\n",
    "* Torch\n",
    "* Caffe/Caffe2\n",
    "* Scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4wE487cOLHkw"
   },
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6pkqwm2_LssA"
   },
   "outputs": [],
   "source": [
    "# Set the logging level, API Doc - https://www.tensorflow.org/api_docs/python/tf/logging/set_verbosity)\n",
    "# This is a low-level setting so more information than you would like may appear\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JT7r0HrVLt56"
   },
   "outputs": [],
   "source": [
    "def get_input_fn(data_set, label, features, num_epochs=None, shuffle=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset(DataFrame): Pandas DataFrame containing the dataset\n",
    "        num_epochs(int): number of epochs to train\n",
    "        shuffle(bool): shuffle the dataset randomly before training\n",
    "    \"\"\"\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "            x=pd.DataFrame({k: data_set[k].values for k in features}),\n",
    "            y=pd.Series(data_set[label].values),\n",
    "            num_epochs=num_epochs,\n",
    "            shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the relevant features and label we are trying to predict; market prices and presidential data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1006
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15278,
     "status": "ok",
     "timestamp": 1523610171100,
     "user": {
      "displayName": "Avi Thaker",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "117542160256971382138"
     },
     "user_tz": 420
    },
    "id": "sUhbMb4yLvxB",
    "outputId": "9db68016-32ec-4ae6-fc64-2a1e03c00a14"
   },
   "outputs": [],
   "source": [
    "# Labeled Data\n",
    "COLUMNS = combined_df.columns[1:13] # Features and wanted predicted label\n",
    "FEATURES = combined_df.columns[2:13] # Features only without predicted label\n",
    "LABEL = \"party\" # Trying to predict the party that will win the election\n",
    "# Feature Columns\n",
    "feature_cols = [tf.feature_column.numeric_column(k) for k in FEATURES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split up the testing and training set. Generally **(almost always!) you would have a test set, and multiple validation sets.** However, given the small sample size we will only train the model, and only predict on the most recent election. Additonally, be sure to randomize your data but consider the time component. In practice there is not always enough data and more data tends to give better performance. After a model has been validated to \"work\" it is retrained on the entire set to hopefully improve the model's performance.\n",
    "\n",
    "__Try for yourself:__ Split up the training_set, test_set, and prediction_set and see how the model performs given different dataset sizes. Predict on other elections, how does the model perform on other elections? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test, Train, Prediction Sets\n",
    "training_set = combined_df[COLUMNS].iloc[:-1] # remove the most recent election results\n",
    "test_set = None # This is not good in practice but doing for now due to low amount of data\n",
    "prediction_set = combined_df[COLUMNS].iloc[-1:] # the most recent election results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we actually build the model, using [DNNRegressor](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor) in Tensorflow. We will build a fully connected network with 4 hidden layers with dimensions 44,22,11,11. (This would be considered a small deep network, look at Resnet 152 - a network 152 layers deep used for image classification tasks). \n",
    "\n",
    "The larger the model the better performance is expected (unless you are overfitting), but runtime will be significantly longer. For very large models, with giant datasets many GPU's are used in parallel to train upon (I had a project which used maybe 1000 at once). \n",
    "\n",
    "An important note is hyperparameter selection. These can be the network architecture, initial starting weights, activation functions, regularization, dropout rate, and many other factors. Right now we are using mainly defaults for simplicity. In practice you would train many models across a large variety of parameters, and select the best one (be careful of overfit). A large portion of a data scientests time is finding the best selection of hyperparameters. \n",
    "\n",
    "Now with model's becoming more common, AutoML/AutoDL is becoming a practice where Deep Networks are used to predict the best parameters to initially use.\n",
    "\n",
    "**__Note__: you will need to delete the model_dir after use or use another directory as this defaults to checkpointing from the last training epoch**, if you change the model params without deleting you may encounter an error, or keep training from the point you left off!\n",
    "\n",
    "Play with the evaluation part (commented out) on a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Fully Connected DNN with 11, 11, units respectively\n",
    "regressor = tf.estimator.DNNRegressor(feature_columns=feature_cols, hidden_units=[44,22,11,11], model_dir=\"./pres\")\n",
    "\n",
    "# Train the network\n",
    "regressor.train(input_fn=get_input_fn(training_set, LABEL, FEATURES), steps=2000)\n",
    "\n",
    "# Evaluate the loss over one epoch of the test_set\n",
    "# ev = regressor.evaluate(input_fn=get_input_fn(test_set,num_epochs=1, shuffle=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "Look at that loss! That is an incredibly low loss given the size of the network with respect to the size of the data. We might have overfit our data. \n",
    "\n",
    "Now predict the output to see how we did!!! Note due to randomness of parameters upon initialization it is expected to get slighlty different results than others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out predictions over a slice of the prediction set\n",
    "y = regressor.predict(input_fn=get_input_fn(prediction_set, LABEL, FEATURES, num_epochs=1, shuffle=False))\n",
    "\n",
    "# .predict() returns an iterator of dicts; convert to a list and print the predictions\n",
    "predictions = list(p[\"predictions\"] for p in itertools.islice(y, len(prediction_set)))\n",
    "print(\"Predictions: {}\".format(str(predictions)))\n",
    "if predictions[0] > 0.5:\n",
    "    print(\"Predicting a TRUMP (Republican) victory\")\n",
    "else:\n",
    "    print(\"Predicting a CLINTON (Democratic) victory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did not change parameters you should get roughly 0.9, meaning we expect the Republican party to win the election, and therefore a Trump victory. **We correctly predicted the election!** However, with an output of 0.9 we are pretty confident, thus may be in a situation where we are overpredicting (depending on the parameters and given that this is a regression the coefficients in the network can lead to predicting over a value of 1). Do not take this value as a likelyhood as this process can be thought of as a regression. You can use a softmax to get predicted probabilities, but that is out of the scope of this workbook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Predicting the Market return after the Trump Election\n",
    "We will follow a similar process to the above. Furthermore, we will use similar features, but take out the party as that would inject some forward looking bias. \n",
    "\n",
    "Here we will predict a direction instead of a size (1 - market up from election, 0 - market down from election). In section 2 we labeled the direction of the market return. You can play with that data column and return instead the market value, see how it changes the performance! Furthermore, play with the other values and add it to the labels to predict upon! See if you can classify the direction of 1 year out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets build a DNN to predict the expected market response, this follows a similar process as above\n",
    "M_COLUMNS = combined_df.columns[2:14]\n",
    "M_FEATURES = combined_df.columns[2:13]\n",
    "M_LABEL = \"day_after_1\"\n",
    "\n",
    "# Test, Train, Prediction Sets\n",
    "m_training_set = combined_df[M_COLUMNS].iloc[:-1]\n",
    "m_test_set = None # This is not good in practice but doing for now due to low amount of data\n",
    "m_prediction_set = combined_df[M_COLUMNS].iloc[-1:] # What is the expected return\n",
    "\n",
    "# Feature Columns\n",
    "m_feature_cols = [tf.feature_column.numeric_column(k) for k in M_FEATURES]\n",
    "\n",
    "# Build a Fully Connected DNN with 11, 11, units respectively\n",
    "m_regressor = tf.estimator.DNNRegressor(feature_columns=m_feature_cols, hidden_units=[44,22,11,11], model_dir=\"./market\")\n",
    "\n",
    "# Train the network\n",
    "m_regressor.train(input_fn=get_input_fn(m_training_set, M_LABEL, M_FEATURES), steps=2000)\n",
    "\n",
    "# Evaluate the loss over one epoch of the test_set\n",
    "#ev = regressor.evaluate(input_fn=get_input_fn(test_set,num_epochs=1, shuffle=False))\n",
    "\n",
    "#Print out predictions over a slice of the prediction set\n",
    "m_y = m_regressor.predict(input_fn=get_input_fn(m_prediction_set, M_LABEL, M_FEATURES, num_epochs=1, shuffle=False))\n",
    "\n",
    "# .predict() returns an iterator of dicts; convert to a list and print the predictions\n",
    "m_predictions = list(p[\"predictions\"] for p in itertools.islice(m_y, len(m_prediction_set)))\n",
    "print(\"Predictions: {}\".format(str(m_predictions)))\n",
    "print(\"Actual: \" + str(m_prediction_set.iloc[0].day_after_1))\n",
    "\n",
    "if round(m_predictions[0][0]) ==  m_prediction_set.iloc[0].day_after_1:\n",
    "    print(\"Correctly predicted the market direction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again it looks like we are in a situation were we may have overpredicted! But we did indeed predict the direction of the market return after Trump's victory! \n",
    "\n",
    "Congrats you have trained your first (or maybe not) neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Remarks\n",
    "This serves as only an introduction to how to use deep learning to predict the market. I would not use the model as is to predict the next election. There is much to be said about data engineering and bias that we can do to improve this model. \n",
    "\n",
    "What are some problems with the process that we have done?\n",
    "\n",
    "What can be done to improve the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"./market\") # Need to update to remove when folders are not empty\n",
    "shutil.rmtree(\"./pres\") # Need to update to remove when folders are not empty"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "trump_election.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
